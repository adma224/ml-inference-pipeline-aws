{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4ef013",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 for Tweet Token Prediction with TensorFlow\n",
    "\n",
    "In this exciting journey, we're leveraging the cutting-edge capabilities of GPT-2, a powerhouse in the NLP realm, to predict the next token in sequences of tweet data. As we dive into this project, we'll be exploring the fascinating world of transformer models, specifically focusing on fine-tuning the GPT-2 model using TensorFlow to enhance its predictive prowess on our dataset of preprocessed tweets.\n",
    "\n",
    "Our goal? To fine-tune a pre-trained GPT-2 model so it becomes adept at predicting the next word in a tweet, harnessing the vast knowledge it has acquired from extensive pre-training. This model will serve as a counterpart to our LSTM model, allowing for an A/B test to determine which model better suits our specific NLP task.\n",
    "\n",
    "Let's embark on this deep learning adventure, equipped with TensorFlow and the Hugging Face Transformers library, to push the boundaries of what's possible with NLP and tweet data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44737abd",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "First, ensure that you have installed the Hugging Face transformers library. If not, you can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5641110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4932c",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Our first step is to prepare our dataset of preprocessed tweets for the GPT-2 model. This involves loading the dataset, ensuring it's in the correct format for tokenization, and then creating TensorFlow datasets for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = 'preprocessed_tweets.csv'\n",
    "tweets_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# For simplicity, we concatenate all tweets into a single text corpus\n",
    "tweets_text = ' '.join(tweets_df['cleaned_text'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab447e88",
   "metadata": {},
   "source": [
    "## Tokenization and TensorFlow Dataset\n",
    "\n",
    "Tokenizing our text data is crucial for transforming it into a format that GPT-2 can understand. We'll then split the tokenized data into training examples and create a TensorFlow dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokens = tokenizer.encode(tweets_text, return_tensors='tf')\n",
    "\n",
    "# Organize our data into TensorFlow datasets\n",
    "# Here, we're making a simple sequence dataset for demonstration purposes\n",
    "SEQ_LENGTH = 128  # Sequence length to train on\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "def map_func(input_ids):\n",
    "    return {'input_ids': input_ids[:-1]}, input_ids[1:]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokens[0])\n",
    "sequences = dataset.batch(SEQ_LENGTH+1, drop_remainder=True)\n",
    "dataset = sequences.map(map_func)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b96d29",
   "metadata": {},
   "source": [
    "## Model Initialization and Fine-tuning\n",
    "\n",
    "Now, we'll load the pre-trained GPT-2 model and prepare it for fine-tuning with our tweet data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7617f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare the model for training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Fine-tune the model\n",
    "EPOCHS = 4\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a5a46",
   "metadata": {},
   "source": [
    "## Saving the Fine-tuned Model\n",
    "\n",
    "After fine-tuning, it's essential to save our model for future use, whether for further training, evaluation, or deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2503a0",
   "metadata": {},
   "source": [
    "## Containerizing the Fine-tuned GPT-2 Model\n",
    "\n",
    "To deploy our fine-tuned GPT-2 model, we'll containerize it using Docker. This involves creating a Dockerfile that specifies the environment and dependencies needed to run our model, building a Docker image based on this Dockerfile, and testing it locally to ensure everything is set up correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7d31e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3256988575.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    FROM tensorflow/serving\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Use TensorFlow Serving image as the base image\n",
    "FROM tensorflow/serving\n",
    "\n",
    "# Copy the fine-tuned model to the container\n",
    "COPY ./fine_tuned_gpt2 /models/gpt2/1\n",
    "\n",
    "# Set environment variables\n",
    "ENV MODEL_NAME=gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38941227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker image\n",
    "docker build -t gpt2-tweet-predictor .\n",
    "\n",
    "# Run the Docker container locally\n",
    "docker run -p 8501:8501 --name=gpt2_model_container gpt2-tweet-predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0c57",
   "metadata": {},
   "source": [
    "## Uploading the Docker Image to Amazon ECR\n",
    "\n",
    "After testing the Docker container locally, the next step is to upload our Docker image to Amazon Elastic Container Registry (ECR) so that it can be deployed on AWS SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507274df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Replace 'your-region' with your AWS region, e.g., 'us-west-2'\n",
    "aws_region = 'your-region'\n",
    "ecr_repository_name = 'gpt2-tweet-predictor'\n",
    "\n",
    "# Create ECR client\n",
    "ecr_client = boto3.client('ecr', region_name=aws_region)\n",
    "\n",
    "# Create an ECR repository\n",
    "response = ecr_client.create_repository(repositoryName=ecr_repository_name)\n",
    "repository_uri = response['repository']['repositoryUri']\n",
    "\n",
    "print(f\"Repository URI: {repository_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda54d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to ECR\n",
    "aws ecr get-login-password --region your-region | docker login --username AWS --password-stdin your-account-id.dkr.ecr.your-region.amazonaws.com\n",
    "\n",
    "# Tag your Docker image with the ECR repository URI\n",
    "docker tag gpt2-tweet-predictor:latest your-account-id.dkr.ecr.your-region.amazonaws.com/gpt2-tweet-predictor:latest\n",
    "\n",
    "# Push the Docker image to ECR\n",
    "docker push your-account-id.dkr.ecr.your-region.amazonaws.com/gpt2-tweet-predictor:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ed3da",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've now fine-tuned a GPT-2 model for tweet token prediction, containerized the model using Docker, and uploaded it to Amazon ECR. This model is ready for deployment on AWS SageMaker, setting the stage for an A/B testing scenario against our LSTM model. The journey from training to deployment showcases the power of modern NLP models and cloud services in bringing AI applications to life.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcde058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf59c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76054c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
